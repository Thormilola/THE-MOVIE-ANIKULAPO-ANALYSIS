{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "efb9b9de",
   "metadata": {},
   "source": [
    "# ANIKULAPO - An Analysis of Twitter's Perception of the Movie Anikulapo\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff7dcddb",
   "metadata": {},
   "source": [
    "### 1.Introduction\n",
    "I scraped over 34,000 tweets from twitter using a social network scraper library in python called snscrape for this analysis project. I also performed a sentiment analysis using TextBlob library in python."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e0faa39",
   "metadata": {},
   "source": [
    "### Contents\n",
    "1. Introduction\n",
    "2. Data Gathering\n",
    "3. Data Assessment and Wrangling\n",
    "4. Data Preprocessing\n",
    "5. Sentiment Analysis\n",
    "6. Data Visualization\n",
    "7. Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5bd9339",
   "metadata": {},
   "source": [
    "### 2. Data Gathering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d6e01b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import snscrape.modules.twitter as sntwitter\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "import string\n",
    "import re\n",
    "\n",
    "import wordcloud\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "import textblob\n",
    "from textblob import textblob\n",
    "\n",
    "from emot.emo_unicode import UNICODE_EMOJI\n",
    "\n",
    "Lemmatizer = WordNetLemmatizer() \n",
    "\n",
    "import warnings\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7cf456b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#scraping the tweet\n",
    "query = '(anikulapo OR anikulapothemovie OR #anikulapo OR #anikulapothemovie) until:2022-10-30 since:2022-09-30'\n",
    "tweets = []\n",
    "\n",
    "for i, tweet in enumerate(sntwitter.TwitterSearchScraper(query).get_items()):\n",
    "    if i > 35000:\n",
    "        break\n",
    "    else:\n",
    "        tweets.append([tweet.date, tweet.user.username, tweet.sourceLabel, tweet.content, tweet.user.location, tweet.likeCount, tweet.retweetCount])\n",
    "df = pd.DataFrame(tweets, columns = ['Date', 'User', 'Source', 'Tweet', 'Location', 'Like_Count', 'Retweet_Count'])\n",
    "df.to_csv('project_anikulapo.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b303e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing the scraped dataset into a dataframe\n",
    "df = pd.read_csv('project_anikulapo.csv', encoding = 'unicode_escape')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "823f9acf",
   "metadata": {},
   "source": [
    "### 3. Data Assessment and Wrangling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e745477",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check first five rows\n",
    "anikulapo.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daeab917",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check the shape \n",
    "anikulapo.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5588cd64",
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking for null values\n",
    "anikulapo.isna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3136950e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking the number of missing value in the whole dataset\n",
    "anikulapo.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdb7e828",
   "metadata": {},
   "outputs": [],
   "source": [
    "#on ascertaining that only the location column had missing value, i replaced null values with unknown\n",
    "anikulapo['Location'] = anikulapo['Location]'.fillna(values = 'Unknown')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be3314d4",
   "metadata": {},
   "source": [
    "### 4. Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4ba8274",
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining a function to extract hashtags and creating a new column for those hashtags\n",
    "def hashtag(Tweet):\n",
    "    tweet = re.findall(r'#\\w+', Tweet)\n",
    "    return ' '.join(tweet)\n",
    "anikulapo['hashtags'] = anikulapo['Tweet'].apply(hashtag)\n",
    "anikulapo.head(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e9d6072",
   "metadata": {},
   "outputs": [],
   "source": [
    "#listing all hashtags\n",
    "hashtags_list = anikulapo['hashtags'].tolist()\n",
    "\n",
    "# Iterate over all hashtags and split where there is more than one hashtag\n",
    "hashtags = []\n",
    "for item in hashtags_list:\n",
    "    item = item.split()\n",
    "    for i in item:\n",
    "        hashtags.append(i)\n",
    "        \n",
    "# Determine Unique count of all hashtags used\n",
    "counts = Counter(hashtags)\n",
    "hashtags_anikulapo = pd.DataFrame.from_dict(counts, orient='index').reset_index()\n",
    "hashtags_anikulapo.columns = ['Hashtags', 'Count']\n",
    "hashtags_anikulapo.sort_values(by='Count', ascending=False, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1946769",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check the top 5 most used hashtags\n",
    "hashtags_anikulapo.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a7415c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining set containing all stopwords in english\n",
    "stop_words_eng = list(stopwords.words('english'))\n",
    "user_stop_words =[\"i\", \"me\", \"my\", \"myself\", \"we\", \"our\", \"ours\", \"ourselves\", \"you\", \"your\", \"yours\", \"yourself\", \n",
    "                   \"yourselves\", \"he\", \"him\", \"his\", \"himself\", \"she\", \"her\", \"hers\", \"herself\", \"it\", \"its\", \"itself\",\n",
    "                   \"they\", \"them\", \"their\", \"theirs\", \"themselves\", \"what\", \"which\", \"who\", \"whom\", \"this\", \"that\", \n",
    "                   \"these\", \"those\", \"am\", \"is\", \"are\", \"was\", \"were\", \"be\", \"been\", \"being\", \"have\", \"has\", \"had\", \n",
    "                   \"having\", \"do\", \"does\", \"did\", \"doing\", \"a\", \"an\", \"the\", \"and\", \"but\", \"if\", \"or\", \"because\", \"as\", \n",
    "                   \"until\", \"while\", \"of\", \"at\", \"by\", \"for\", \"with\", \"about\", \"against\", \"between\", \"into\", \"through\", \n",
    "                   \"during\", \"before\", \"after\", \"above\", \"below\", \"to\", \"from\", \"up\", \"down\", \"in\", \"out\", \"on\", \"off\", \n",
    "                   \"over\", \"under\", \"again\", \"further\", \"then\", \"once\", \"here\", \"there\", \"when\", \"where\", \"why\", \"how\",\n",
    "                   \"all\", \"any\", \"both\", \"each\", \"few\", \"more\", \"most\", \"other\", \"some\", \"such\", \"no\", \"nor\", \"not\",\n",
    "                   \"only\", \"own\", \"same\", \"so\", \"than\", \"too\", \"very\", \"s\", \"t\", \"can\", \"will\", \"just\", \"don\", \"should\", \n",
    "                   \"now\",\"anyone\",\"today\",\"yesterday\",\"day\", \"already\"]\n",
    "stop_words = stop_words_eng + user_stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb6f798b",
   "metadata": {},
   "outputs": [],
   "source": [
    "emoji = list(UNICODE_EMOJI.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a7e22b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocessing tweet for sentiment analysis\n",
    "def preprocessedTweets(Tweet):    \n",
    "#converting tweets to lowercase characters\n",
    "    tweet = Tweet.lower()\n",
    "#cleaning and removing links and URLs\n",
    "    tweet = ' '.join(re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t]) |(\\w+:\\/\\/\\S+)\", \" \", tweet).split())   \n",
    "#still cleaning, removing mentions and repeating characters\n",
    "    tweet = re.sub(r'\\@\\w+|\\#\\w+|\\d+', '', tweet)\n",
    "#cleaning special characters\n",
    "    tweet = re.sub(r'[^\\x00-\\x7F]+', '', tweet)\n",
    "#removing punctuations and numbers\n",
    "    punct = str.maketrans('', '', string.punctuation+string.digits)\n",
    "    tweet = tweet.translate(punct)\n",
    "#cleaning, tokenizing, stopword removal\n",
    "    tokens = word_tokenize(tweet)\n",
    "    filtered_words = [w for w in tokens if w not in stop_words]\n",
    "    filtered_words = [w for w in filtered_words if w not in emoji]\n",
    "#lemmatizing words\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemma_words = [lemmatizer.lemmatize(w) for w in filtered_words]\n",
    "    tweet = ' '.join(lemma_words)\n",
    "    return tweet\n",
    "df['Processed_Tweets'] = df['Tweet'].apply(preprocessedTweets)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d43ff17",
   "metadata": {},
   "source": [
    "### 5. Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e94e65a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def polarity(tweet):\n",
    "    return TextBlob(tweet).sentiment.polarity\n",
    "\n",
    "#define function to get polarity\n",
    "def sentimenttextblob(polarity):\n",
    "    if polarity < 0:\n",
    "        return \"Negative\"\n",
    "    elif polarity == 0:\n",
    "        return \"Neutral\"\n",
    "    else:\n",
    "        return \"Positive\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87bb74a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Polarity'] = df['Processed_Tweet9'].apply(polarity)\n",
    "df['Sentiment'] = df['Polarity'].apply(sentimenttextblob)\n",
    "df['Sentiment'].value_counts()\n",
    "df.tail(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb3cd383",
   "metadata": {},
   "source": [
    "### 6. Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89ef8bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_long_string = df['Processed_Tweets'].tolist()\n",
    "tweets_long_string = \" \".join(tweets_long_string)\n",
    "\n",
    "tweet_wc = WordCloud(collocations = False, max_words = 200, background_color = 'White').generate(tweets_long_string)\n",
    "\n",
    "plt.imshow(tweet_wc, interpolation = 'bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d4ccf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save final file\n",
    "df.to_csv(\"anikulapo_final_file.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52cd076b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_wc.to_file(\"wordcloud.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0798abb1",
   "metadata": {},
   "source": [
    "### 7. Conclusion\n",
    "Exported this file to powerBI to create a better visualization for my analysis."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
